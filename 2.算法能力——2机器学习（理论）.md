# 理论基础

## 数据挖掘与机器学习

## 训练集/验证集/测试集

```python
"""
	训练集用来训练，构建模型
	验证集是用来模型训练阶段测试模型的好坏
	等模型训练好之后，再用测试集来评估模型的好坏
"""
```



## 监督学习/无监督学习/半监督学习/强化学习

```python
"""
	监督学习：有标签
	无监督学习：无标签
	半监督学习：监督学习和无监督学习相结合的一种学习方式。主要是用来解决使用少量带标签的数据和大量没有标签的数据进行训练和分类的问题
	强化学习：智能系统在环境的连续互动中学习最优行为策略的机器学习问题。
"""
```



## 模型分类

```python
"""
	1.概率模型与非概率模型
		（1）概率模型		P(y|x)
			常见算法：
				决策树、朴素贝叶斯、概率图模型（隐马尔可夫模型、条件随机场、高斯混合模型）等
		（2）非概率模型	y=f(x)
			常见算法：
				感知机、支持向量机、k近邻、k均值聚类、AdaBoost、神经网络等
		（3）逻辑回归既可以看作是概率模型又可以看作是非概率模型。
		
	2.线性模型与非线性模型
	
	3.参数化模型与非参数化模型
		（1）参数化模型假设模型参数的维度固定，模型可以随由有限维参数完全刻画。
		（2）非参数模型假设模型参数的维度不固定或者说无穷大，随数据量的增加而不端增大。
"""
```

## 模型评估与模型选择

## 过拟合

## 优化算法

### 损失函数（代价函数）

### 梯度下降法

```python
"""
	梯度下降法：求解无约束最优化问题的一种最常用的方法，是迭代算法。
	
	当目标函数是凸函数时，梯度下降的解是全局最优解；
	同理，若目标函数不是凸函数时，那么梯度下降的解存在局部最优解
"""
```

### 随机梯度下降

### 正则化

### EM算法

### 反向传播

### Momentum

### AdaGrad

### Adam

## 常见应用

```python
"""
	分类：图像识别、垃圾邮件分类、文本分类、...（预测数据为类别型数据，并且类别已知）
	回归：房价预测、...（预测数据为连续型数值）
	聚类：用户分类、...（预测数据为类别型数据，但是类别未知）
"""
```

------

# 

------

# 机器学习算法

## 感知机 P

## 线性回归 LR

```python
"""
	回归分析：用来建立方程模拟两个变量或多个变量之间如何关联
	一元线性回归：
	多元线性回归：
	岭回归：
	Lasso回归：
	
	涉及知识点：代价函数、最小二乘法、梯度下降法
"""
```

> ## 一元线性回归

```python
"""
	
"""
```



> ## 多元线性回归
>
> ## 岭回归

> ## Lasso回归

## 逻辑回归 LR

## 决策树 DT

```python
"""
	（1）寻找最优决策树是一个NP完全问题，决策树的这一特点，说明我们无法利用计算机在多项式时间内，找出全局最优的解。
	（2）在实际中我们通常采用启发式学习的方法去构建一颗满足启发式条件的决策树。
	（3）决策树由结点和有向边组成。结点包含内部结点（表示特征或属性）和叶结点（表示一个类）。
	（4）决策树还表示给定特征条件下类的条件概率分布。决策树学习的本质是从训练数据集中归纳出一组分类规则。
	
	涉及知识点：信息论，树的数据结构，优化理论。
	建议：学完决策树，建议可以直接去学随机森林
"""
```

​	**决策树**的学习包含了***特征选择***、***树的生成***、***树的剪枝***三个过程。对决策树剪枝的目的是为了降低模型的复杂度，同时减缓`过拟合现象`，使它具有更好的`泛化能力`。

> ### 特征选择

```python
"""
	（1）特征选择在于选取对训练数据具有分类能力的特征。
	（2）特征选择的准则通常有：信息增益（ID3算法）、信息增益比（C4.5算法）和基尼指数（CART算法）
	（3）以上准则涉及了信息论的一些概念（设X，Y为随机变量）
		（3.1）熵：表示随机变量不确定性的度量，H(X)=-Sum(plogp),底数为2或e
		（3.2）条件熵：定义为给定X的条件下Y的条件概率分布的熵对X的数学期望，H(Y|X)=Sum(pH(Y|X))。
		（3.3）当以上两个概念中的概率p是通过数据估计（极大似然估计）得到时，所对应为经验熵和经验条件熵
		（3.4）如果有0概率，令0log0=0
"""
```

> 信息增益

```python
"""
	定义：特征A对数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A在给定条件下D的条件经验熵H(D|A)之差
		即，g(D,A) = H（D） - H(D|A)
		
	（1）信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度；
	（2）信息增益等价于训练数据集中类与特征的互信息；
	（3）根据信息增益准则选择的特征方法是，选择信息增益最大的特征；
"""
```

![](E:\2---求职\附件\信息增益.png)

> 信息增益比
>

```python
"""
	定义：如下所示
"""
```

![](E:\2---求职\附件\信息增益比.png)

​									其中，n是特征A取值的个数

> 基尼指数—分类树的选择准则
>

![](E:\2---求职\附件\基尼指数1.png)

![](E:\2---求职\附件\基尼指数2.png)

> ### 树的生成

> ID3算法
>

```python
"""
	ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树。
	
	ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。
"""
```

![](E:\2---求职\附件\ID3算法1.png)

![ID3算法2](E:\2---求职\附件\ID3算法2.png)

> C4.5算法
>

```python
"""
	C4.5算法与ID3算法类似，用信息增益比准则选择特征，递归的构建决策树。
	
	C4.5算法也只有树的生成，没有剪枝
"""
```

![](E:\2---求职\附件\C4.5算法.png)

> CART算法
>

```python
"""
	CART模型 Classification And Regression Tree模型
	CART假设决策树是二叉树
	
	CART的回归树生成采用平方误差最小化准则，进行特征选择
	CART的分类树用基尼指数最小化准则，进行特征选择
	
	CART算法由以下两部组成：
	（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大
	（2）决策树剪枝：用验证数据集对已生成的决策树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。
"""
```

3.1 回归树的生成

![](E:\2---求职\附件\CART回归树算法.png)

3.2 分类树的生成

![](E:\2---求职\附件\CART分类树算法1.png)

![CART分类树算法2](E:\2---求职\附件\CART分类树算法2.png)

> ### 树的剪枝

```python
"""
	决策树生成算法递归的产生决策树，这样的树往往对训练数据分类准确，但是对未知的测试数据的分类却没有那么准确，容易出现过拟合现象。
	因此，将已生成的树进行简化的过程就叫做剪枝（pruning）
	
	（1）预剪枝：在生成决策树的过程中提前停止树的增长
		（1.1）预剪枝对于何时停止树的生长有以下几种方法
			（1.1.1）当树达到一定深度的时候，停止树的生长；
			（1.1.2）当到达当前结点的样本数量小于某个阈值的时候，停止树的生长；
			（1.1.3）计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。
		（1.2）预剪枝适合解决大规模问题，但是又欠拟合的风险
		
	（2）后剪枝：在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树
		（2.1）后剪枝方法通常可以得到泛化能力更强的决策树，但是时间开销会更大。
"""
```

1. 通过极小化决策树整体的损失函数或代价函数进行剪枝

   ![](E:\2---求职\附件\决策树剪枝算法1.png)

   ![决策树剪枝算法2](E:\2---求职\附件\决策树剪枝算法2.png)

   ![](E:\2---求职\附件\树的剪枝算法1.png)

   ![树的剪枝算法2](E:\2---求职\附件\树的剪枝算法2.png)

2. CART剪枝

```python
"""
	CART剪枝算法分两步：
	（1）首先从生成算法产生的决策树T0底端开始不断剪枝，直到T0的根节点。形成一个子树序列{T0,T1,...Tn};
	（2）然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。
"""
```

![](E:\2---求职\附件\CART剪枝算法.png)

> 三种算法的分析比较

```python
"""
	（1）ID3采用信息增益最为评价标准，倾向于取值较多的特征，分类的泛化能力弱；
	（2）C4.5对ID3进行优化，引入信息增益比一定程度上对取值较多的特征进行惩罚，避免ID出现过拟合的特性，提
	升决策树的泛化能力
	（3）从样本类型的角度，ID3只能处理离散变量，而C4.5和CART都可以处理连续型变量。
	（4）从应用角度，ID3和C4.5只能用于分类任务，而CART即可以用于分类又可以应用于回归任务。
	（5）ID3对样本特征的缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；
	（6）ID3和C4.5可以在每个结点上产生多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个
	分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用；
	（7）ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比
"""
```



## K近邻 KNN

## 支持向量机 SVM

## 概率图模型 PGM

### 贝叶斯网络 BN

### 隐马尔可夫模型 HMM

### 马尔科夫网络 MN

### 条件随机场 CRF

### 最大熵模型 MEM



## 集成学习

```python
"""
	比起算法，集成学习更像是一种思想，集各家之长为策略。
	（1）集成学习就是组合多个学习器，最后可以得到一个更好的学习器。
	（2）集成学习是一大类模型融合策略和方法的统称。
"""
```

```python
# 集成学习一般可以分为以下3个步骤：
"""
	（1）找到误差互相独立的基分类器（确定基分类器）
	（2）训练基分类器
	（3）合并基分类器的结果
"""


# 合并基分类器的方法有两种
"""
	voting（投票法）
		（1）用投票的方式，将获得最多选票的结果作为最终的结果
	stacking（学习法）
		（1）采用串行的方式，把前一个分类器的结果输出到下一个分类器，
		（2）将所有基分类器的输出结果相加（或者用更复的算法融合），作为最终结果的输出
"""


# 常用的基分类器是什么？
"""
	最常用的是基分类器是决策树，主要有以下3个原因：
	（1）决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重；
	（2）决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；
	（3）数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为分类器。
	
	除了决策树外，神经网络模型也适合作为分类器，主要由于神经网络模型也比较“不稳定”，而且还可以通过调整神经元数量，连接方式、网络层数、初始权值等方式引入随机性。
"""


# 偏差与方差
"""
	（1）在有监督学习中，模型的泛化误差来源于两个方面——偏差和方差
	（2）偏差：由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差
	（3）方差：由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。
	（4）方差通常是由于模型的复杂度相对于训练样本数m过高导致的。
"""
```



### Bagging

```python
"""
	个体学习器之间不存在强依赖关系，装袋（bagging）
	
	（1）Bagging也叫做bootstrap aggregating（再抽样），是一种有放回的抽样；
	（2）Bagging进行并行训练，各基分类器之间无强依赖
	（3）Bagging更像是一个集体决策的过程，每个个体单独进行学习，学习内容可以不同和相同，也可以部分重叠。
	（4）Bagging通过投票的方式做出最后的集体决策
"""
```

> 随机森林 RF：基于决策树基分类器的算法

```python
"""
	随机森林 = 决策树 + Bagging + 随机属性选择
"""
```

![1560070384292](E:\2---求职\附件\随机森林算法流程.png)



### Boosting

```python
"""
	个体学习器之间存在强依赖关系，提升（Boosting）
	
	（1）Boosting采用串行的方式训练基分类器，各个分类器之间有依赖；
	（2）基本思想是将基分类器层层叠加，每一层在训练时，对前一层分类器分错的样本，给予更高的权重；
	（3）测试时，根据各层分类器的结果的加权得到最终的结果。
	（4）Boosting算法要求基分类器能对特定的数据分布进行学习。
	
	对于Boosting方法，需要回答两个问题：
	（1）每一轮如何改变训练数据的权值或概率分布
	（2）如何将弱分类器器组合成一个强分类器
"""
```

> #### 自适应提升  AdaBoost

```python
"""
	针对Boosting方法的两个问题，AdaBoost的解决策略是
	（1）提高那些前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值
	（2）采取加权多数表决的方法
		（2.1）加大分类误差率小的弱分类器的权值，使其在表决中起到较大的作用
		（2.2）减小分类误差率大的弱分类器的权值，使其在表决中起到较小的作用
		
	AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据
"""
```

![](E:\2---求职\附件\AdaBoost算法1.png)

![](E:\2---求职\附件\AdaBoost算法2.png)

![](E:\2---求职\附件\AdaBoost算法3.png)

![](E:\2---求职\附件\AdaBoost算法4.png)

```python
"""
	AdaBoost算法解释
"""
```

> #### 提升树BT

```python
"""
	提升树是以分类树或回归树为基本分类器的提升方法，被认为是统计学习方法中性能最好的方法之一。
	
	（1）对于二分类问题，提升树算法只需要将AdaBoost算法中的基分类器限制为二分类树即可，可以说这时的提升树算法时AdaBoost算法的特殊情况
	
	（2）对于回归问题，采用平方误差损失函数
	
	（3）对于使用一般损失函数的一般决策问题，引入梯度提升决策树
"""
```

> #### 梯度提升决策树 GBDT

> #### XGBoost

### 拓展

```python
# --------------------------------------------------------------------
# 			如何从减小方差和偏差的角度解释Boosting和Bagging的原理？
# -------------------------------------------------------------------
"""
	Bagging能够提高弱分类器性能的原因是降低了方差
	Boosting能够提高弱分类器性能的原因是降低了偏差
"""
```

## 降维

### 线性判别分析

### 主成分分析

## 聚类

### 层次聚类

### K均值聚类



## 采样

------

# 

------

# 深度学习算法

## 前向神经网络

## 循环神经网络

## 优化技巧

## 强化学习

## 生成对抗网络